name: Deploy Ollama API with Cloudflare Tunnel

on:
  workflow_dispatch:

jobs:
  serve-ollama:
    runs-on: ubuntu-latest

    steps:
      # ------------------------------
      # 1) Pull source
      # ------------------------------
      - name: Checkout Code
        uses: actions/checkout@v4

      # ------------------------------
      # 3) Install Ollama
      # ------------------------------
      - name: Install & Start Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          sudo mkdir -p /usr/share/ollama/.ollama
          sudo chown -R $USER:$USER /usr/share/ollama/.ollama
          export OLLAMA_ORIGINS="*"
          export OLLAMA_HOST="0.0.0.0"
          nohup ollama serve > ollama.log 2>&1 &

      # ------------------------------
      # 4) Pull Model (skip if cached)
      # ------------------------------
      - name: Pull
        run: |
          echo "Cache MISS â†’ pulling model..."
          ollama pull translategemma:12b
          ollama pull translategemma:4b

      - name: Install Ngrok
        run: |
          curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \
            | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null
          echo "deb https://ngrok-agent.s3.amazonaws.com bookworm main" \
            | sudo tee /etc/apt/sources.list.d/ngrok.list
          sudo apt update
          sudo apt install -y ngrok
          ngrok config add-authtoken ${{ secrets.NGROK_TOKEN }}
          nohup ngrok http 11434 --domain=${{ secrets.NGROK_DOMAIN }} > ngrok.log 2>&1 &

      # ------------------------------
      # 7) Print Ollama Log Forever
      # ------------------------------
      - name: Run Server
        run: |
          echo "Model: TranslateGemma:27b"
          tail -f ollama.log
